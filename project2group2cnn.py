# -*- coding: utf-8 -*-
"""Project2Group2CNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/happyemanaloto/Project2-G2/blob/main/Project2Group2CNN.ipynb

#!/usr/bin/env python
# coding: utf-8
# # %pip install tensorflow matplotlib seaborn scikit-learn
# C:\Users\happy\Documents\ironhack\Week7\Project2-G2>pylint cleaned_cnn_script.py --disable=E1101,E0611,E0401

# -------------------------------------------------------------------
# Your code has been rated at 10.00/10 (previous run: 6.92/10, +3.08)
'''
Train and evaluate a CNN on CIFAR-10 using transfer learning with MobileNetV2.
Includes preprocessing, model training, evaluation, fine-tuning, and saving.
'''
"""

# Import necessary libraries
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix
import tensorflow as tf
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.models import Model
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout
from tensorflow.keras.optimizers import Adam

# Load CIFAR-10 dataset
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()
class_names = [
    "airplane",
    "automobile",
    "bird",
    "cat",
    "deer",
    "dog",
    "frog",
    "horse",
    "ship",
    "truck",
]

# One-hot encode labels
y_train_cat = tf.keras.utils.to_categorical(y_train, 10)
y_test_cat = tf.keras.utils.to_categorical(y_test, 10)

# Set image size and batch size
IMG_SIZE = 160
BATCH_SIZE = 32

# Image preprocessing function
def preprocess(image, name):
    """
    Resize image to target size and normalize pixel values to [0, 1].
    """
    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))
    image = tf.cast(image, tf.float32) / 255.0
    return image, name

# Build TensorFlow Datasets
train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train_cat))
train_ds = (
    train_ds.map(preprocess).shuffle(1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
)

val_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test_cat))
val_ds = val_ds.map(preprocess).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

# Visualize sample images
plt.figure(figsize=(10, 6))
for i in range(10):
    img = x_train[i]
    label = class_names[y_train[i][0]]
    plt.subplot(2, 5, i + 1)
    plt.imshow(img)
    plt.title(label)
    plt.axis("off")
plt.tight_layout()
plt.show()

# Load base MobileNetV2 model with pre-trained weights
base_model = MobileNetV2(
    weights="imagenet", include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3)
)
base_model.trainable = False  # freeze base layers

# Add custom classification head
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(128, activation="relu")(x)
x = Dropout(0.3)(x)
output = Dense(10, activation="softmax")(x)
model = Model(inputs=base_model.input, outputs=output)

# Compile the model
model.compile(
    optimizer="adam",
    loss="categorical_crossentropy",
    metrics=["accuracy"],
)

# Print model summary
model.summary()

# Train the model
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=10,
    callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)],
)

# Evaluate the model on the test set
loss, acc = model.evaluate(val_ds)
print(f"Test Accuracy: {acc:.4f}")

# Predict
y_pred_probs = model.predict(val_ds)
y_pred = np.argmax(y_pred_probs, axis=1)
y_true = np.argmax(y_test_cat, axis=1)

# Report
print(classification_report(y_true, y_pred, target_names=class_names))

# Confusion Matrix
cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(10, 8))
sns.heatmap(
    cm,
    annot=True,
    fmt="d",
    xticklabels=class_names,
    yticklabels=class_names,
    cmap="Blues",
)
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

#Mount google drive
from google.colab import drive
drive.mount('/content/drive')

# Save the model
model.save('cifar10_base_model.keras')

#Verify saving
import os
drive_path = '/content/'
for item in os.listdir(drive_path):
    print(item)

# Unfreeze from layer 100 onwards (out of ~154)
UNFREEZE_FROM = 100

# Unfreeze the base model
base_model.trainable = True

# Freeze all layers before `UNFREEZE_FROM`
for i, layer in enumerate(base_model.layers):
    layer.trainable = i >= UNFREEZE_FROM

# Re-compile model for fine-tuning with lower learning rate
model.compile(
    optimizer=Adam(learning_rate=1e-5),
    loss="categorical_crossentropy",
    metrics=["accuracy"],
)

# Train again with fine-tuning
history_finetune = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=10,
    callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)],
)

model.save('cifar10_transfer_model.keras')

#Verify saving
import os
drive_path = '/content/'
for item in os.listdir(drive_path):
    print(item)